{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping Dialogue Games\n",
    "### Disclaimer\n",
    "This notebook closely follows the tutorial steps from **docs/howto_prototype_games.ipynb** in the [**clp-research/clembench**](https://github.com/clp-research/clembench) repository. <br> \n",
    "While the core content is derived from the original tutorial, some additional detailed descriptions and explanations have been added to serve as a source of knowledge during the development of our project.\n",
    "### Step 1: Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from string import Template\n",
    "\n",
    "# Setting the correct directory so that import statements do not fail\n",
    "# If you are unsure about the directory, just type pwd in your terminal\n",
    "sys.path.append('/home/zoltan/Desktop/Gruppe-C-PRO2-Projekt/clembench-main')\n",
    "\n",
    "from backends import Model, get_model_for, load_model_registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Specifying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the default model registry from the backends folder\n",
    "# You can inspect the registry when opening backends/model_registry.json\n",
    "load_model_registry()\n",
    "\n",
    "# Specify the model\n",
    "THIS_MODEL = 'gpt-4o-mini-2024-07-18'\n",
    "llm: Model = get_model_for(THIS_MODEL)\n",
    "\n",
    "# Set generation arguments\n",
    "# temperature: the lower it is, the more deterministic the output becomes\n",
    "# max_tokens: the amount of tokens/words the model is allowed to use in a response\n",
    "llm.set_gen_args(temperature=0.0, max_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Adding Messages\n",
    "This function appends a new message to the `context`, which is a list of dictionaries where each dictionary represents a message in the conversation. The `sysmsg` is called if no conversation has been initialized yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(context, msg, role='user', sysmsg = 'You are a player in a game'):\n",
    "    if context == []:\n",
    "        context = [\n",
    "            {\"role\": \"system\", \"content\": sysmsg}\n",
    "        ]\n",
    "    context.append({\"role\": role, \"content\": msg})\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a player in a game'},\n",
       " {'role': 'user', 'content': 'How old is the sun'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding a message, we can see the resulting lis of dictionaries\n",
    "add_message([], \"How old is the sun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Sun is approximately 4.6 billion years old. This age is estimated based on the study of solar system formation and the ages of the oldest meteorites.\n"
     ]
    }
   ],
   "source": [
    "prompt = add_message([], \"How old is the sun?\")\n",
    "# resp_text = llm.generate_response(prompt)   # Returns the entire tuple from generate_response\n",
    "prompt, resp, resp_text = llm.generate_response(prompt) # Returns only the contents of the generated result\n",
    "print(resp_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the complete structure of the returned tuple, where it is easier to see each detail:\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"content\": [\n",
    "      {\n",
    "        \"text\": \"You are a player in a game\",\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    ],\n",
    "    \"role\": \"system\"\n",
    "  },\n",
    "  {\n",
    "    \"content\": [\n",
    "      {\n",
    "        \"text\": \"How old is the sun?\",\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    ],\n",
    "    \"role\": \"user\"\n",
    "  }\n",
    "],\n",
    "{\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"finish_reason\": \"stop\",\n",
    "      \"index\": 0,\n",
    "      \"logprobs\": null,\n",
    "      \"message\": {\n",
    "        \"content\": \"The Sun is approximately 4.6 billion years old. This age is estimated based on the study of solar system formation and the ages of the oldest meteorites.\",\n",
    "        \"function_call\": null,\n",
    "        \"refusal\": null,\n",
    "        \"role\": \"assistant\",\n",
    "        \"tool_calls\": null\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"created\": 1724755685,\n",
    "  \"id\": \"chatcmpl-A0o21VxrNbI1bSALI1bpUA1XFE3qG\",\n",
    "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"system_fingerprint\": \"fp_507c9469a1\",\n",
    "  \"usage\": {\n",
    "    \"completion_tokens\": 33,\n",
    "    \"prompt_tokens\": 24,\n",
    "    \"total_tokens\": 57\n",
    "  }\n",
    "},\n",
    "\"The Sun is approximately 4.6 billion years old. This age is estimated based on the study of solar system formation and the ages of the oldest meteorites.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping a game\n",
    "There are two important elements that need to be considered:\n",
    "##### MOVE_RULE\n",
    "this somewhat defines, what a valid move (what form it takes) is\n",
    "\n",
    "##### GAME_RULE\n",
    "This contains rules that determine what effect a valid move from above should have on the game state, whether a move is legal, whether it updates the game state to `finished`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Game\n",
    "Player A and B take turns, while they have a conversation. Every first word has to start with the next letter of the alphabet, based on the first word of the previous turn and so on, while sticking to a specific topic.\n",
    "\n",
    "\n",
    "### Player A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_A = Template('''Let us play a game. I will tell you a topic and you will give me a short sentence that fits to this topic.\n",
    "But I will also tell you a letter. The sentence that you give me has to start with this letter.\n",
    "After you have answered, I will give you my reply, which will start with the letter following your letter in the alphabet.\n",
    "Then it is your turn again, to produce a reply starting with the letter following that one. And so on. Let's go.\n",
    "Start your utterance with I SAY: and do no produce any other text.\n",
    "The topic is: $topic\n",
    "The letter is: $letter''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let us play a game. I will tell you a topic and you will give me a short sentence that fits to this topic.\\nBut I will also tell you a letter. The sentence that you give me has to start with this letter.\\nAfter you have answered, I will give you my reply, which will start with the letter following your letter in the alphabet.\\nThen it is your turn again, to produce a reply starting with the letter following that one. And so on. Let's go.\\nStart your utterance with I SAY: and do no produce any other text.\\nThe topic is: clothes\\nThe letter is: b\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic = 'clothes'\n",
    "letter = 'b'\n",
    "# Substitute here goes into the init_prompt_A template and replaces $topic and $letter with the variables defined above\n",
    "# This will be logic every single time we call this below\n",
    "init_prompt_A.substitute(topic=topic, letter=letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happens here?\n",
    "So now the initial prompt is defined, along with the topic and the starting letter. <br> Understanding what is happening here is easier, if you think about this part as the start of a conversation with a language model like **ChatGPT**. <br> `init_prompt_A` would basically be your first message to ChatGPT. See: <br> <br> \n",
    "\n",
    "<img src=\"../../utils/gpt_first_prompt.png\" width=\"1200\" style=\"border: 2px solid black;\" />\n",
    "\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Our goal with the game is that we implement a GameMaster that includes `GAME_RULE` and `MOVE_RULE`, which allow us to actually control the flow of the game <br>\n",
    "We could not do that, if we were to just continue the game with ChatGPT in the picture above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I SAY: Blue jeans are a timeless wardrobe staple.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_A, resp, resp_text = llm.generate_response(add_message([], init_prompt_A.substitute(topic='clothes', letter='b')))\n",
    "resp_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding MOVE_RULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we defined in the initial prompt, what our LM returns, has to start with the prefix 'I SAY'\n",
    "# If it is not the case, the function returns False = Wrong guess (?)\n",
    "prefix = 'I SAY: '\n",
    "def parse_reply(text, prefix=prefix):\n",
    "    if not text.startswith(prefix):\n",
    "        return False\n",
    "    return text[len(prefix):]\n",
    "    # The text is returned, but without the prefix (the length of the prefix is 'cut' from the string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what `parse_reply` returns. <br>\n",
    "If we have a sentence without `'I SAY'`, that means, the prompt worked and the LM produced a valid guess.\n",
    "<br> If we see **False**, then the guess is incorrect, or the LM did not understand the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Blue jeans are a timeless wardrobe staple.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_reply(resp_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding GAME_RULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function checks, whether the first letter of the first word in the tokenized text is the same as the 'letter' argument\n",
    "def check_move(text, letter):\n",
    "    token = text.split()\n",
    "    if token[0][0].lower() == letter: # and token[-1][0].lower() == letter:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_move(parse_reply(resp_text), letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player B:\n",
    "This part is pretty self-explanatory, the prompt is a bit different, than what is given to Player A, as there was an initial turn already. <br> \n",
    "That means we already have a sentence that has to be passed on to player B as part of this prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_B = Template('''Let us play a game. I will give you a sentence.\n",
    "The first word in my sentence starts with a certain letter.\n",
    "I want you to give me a sentence as a reply, with the same topic as my sentence, but different from my sentence.\n",
    "The first word of your sentence should start with the next letter in the alphabet from the letter my sentence started with.\n",
    "Let us try to have a whole conversation like that.\n",
    "Please start your reply with 'I SAY:' and do not produce any other text.\n",
    "Let's go.\n",
    "My sentence is: $sentence\n",
    "What do you say?''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let us play a game. I will give you a sentence.\\nThe first word in my sentence starts with a certain letter.\\nI want you to give me a sentence as a reply, with the same topic as my sentence, but different from my sentence.\\nThe first word of your sentence should start with the next letter in the alphabet from the letter my sentence started with.\\nLet us try to have a whole conversation like that.\\nPlease start your reply with 'I SAY:' and do not produce any other text.\\nLet's go.\\nMy sentence is: Blue jeans are a timeless wardrobe staple.\\nWhat do you say?\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = parse_reply(resp_text) # once again, the first response is validated and passed on as the argument 'sentence'\n",
    "prompt_B = init_prompt_B.substitute(sentence=sentence)\n",
    "prompt_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I SAY: Classic denim can be styled in countless ways for any occasion.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now an actual response is generated. The functionality here is the same as for Player A\n",
    "prompt_B, resp_B, resp_B_text = llm.generate_response(add_message([], prompt_B))\n",
    "resp_B_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Classic denim can be styled in countless ways for any occasion.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Response is validated and the prefix 'I SAY' gets ommited.\n",
    "parse_reply(resp_B_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Game States"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
