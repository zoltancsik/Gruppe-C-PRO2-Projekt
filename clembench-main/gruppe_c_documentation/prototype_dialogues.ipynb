{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping Dialogue Games\n",
    "### Disclaimer\n",
    "This notebook closely follows the tutorial steps from **docs/howto_prototype_games.ipynb** in the [**clp-research/clembench**](https://github.com/clp-research/clembench) repository. <br> \n",
    "While the core content is derived from the original tutorial, some additional detailed descriptions and explanations have been added to serve as a source of knowledge during the development of our project.\n",
    "### Step 1: Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from string import Template\n",
    "\n",
    "# Setting the correct directory so that import statements do not fail\n",
    "# If you are unsure about the directory, just type pwd in your terminal\n",
    "sys.path.append('/home/zoltan/Desktop/Gruppe-C-PRO2-Projekt/clembench-main')\n",
    "\n",
    "from backends import Model, get_model_for, load_model_registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Specifying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the default model registry from the backends folder\n",
    "# You can inspect the registry when opening backends/model_registry.json\n",
    "load_model_registry()\n",
    "\n",
    "# Specify the model\n",
    "THIS_MODEL = 'gpt-4o-mini-2024-07-18'\n",
    "llm: Model = get_model_for(THIS_MODEL)\n",
    "\n",
    "# Set generation arguments\n",
    "# temperature: the lower it is, the more deterministic the output becomes\n",
    "# max_tokens: the amount of tokens/words the model is allowed to use in a response\n",
    "llm.set_gen_args(temperature=0.0, max_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Adding Messages\n",
    "This function appends a new message to the `context`, which is a list of dictionaries where each dictionary represents a message in the conversation. The `sysmsg` is called if no conversation has been initialized yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(context, msg, role='user', sysmsg = 'You are a player in a game'):\n",
    "    if context == []:\n",
    "        context = [\n",
    "            {\"role\": \"system\", \"content\": sysmsg}\n",
    "        ]\n",
    "    context.append({\"role\": role, \"content\": msg})\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a player in a game'},\n",
       " {'role': 'user', 'content': 'How old is the sun'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding a message, we can see the resulting lis of dictionaries\n",
    "add_message([], \"How old is the sun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Sun is approximately 4.6 billion years old. This age is estimated based on the study of solar system formation and the ages of the oldest meteorites.\n"
     ]
    }
   ],
   "source": [
    "prompt = add_message([], \"How old is the sun?\")\n",
    "# resp_text = llm.generate_response(prompt)   # Returns the entire tuple from generate_response\n",
    "prompt, resp, resp_text = llm.generate_response(prompt) # Returns only the contents of the generated result\n",
    "print(resp_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the complete structure of the returned tuple, where it is easier to see each detail:\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"content\": [\n",
    "      {\n",
    "        \"text\": \"You are a player in a game\",\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    ],\n",
    "    \"role\": \"system\"\n",
    "  },\n",
    "  {\n",
    "    \"content\": [\n",
    "      {\n",
    "        \"text\": \"How old is the sun?\",\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    ],\n",
    "    \"role\": \"user\"\n",
    "  }\n",
    "],\n",
    "{\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"finish_reason\": \"stop\",\n",
    "      \"index\": 0,\n",
    "      \"logprobs\": null,\n",
    "      \"message\": {\n",
    "        \"content\": \"The Sun is approximately 4.6 billion years old. This age is estimated based on the study of solar system formation and the ages of the oldest meteorites.\",\n",
    "        \"function_call\": null,\n",
    "        \"refusal\": null,\n",
    "        \"role\": \"assistant\",\n",
    "        \"tool_calls\": null\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"created\": 1724755685,\n",
    "  \"id\": \"chatcmpl-A0o21VxrNbI1bSALI1bpUA1XFE3qG\",\n",
    "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"system_fingerprint\": \"fp_507c9469a1\",\n",
    "  \"usage\": {\n",
    "    \"completion_tokens\": 33,\n",
    "    \"prompt_tokens\": 24,\n",
    "    \"total_tokens\": 57\n",
    "  }\n",
    "},\n",
    "\"The Sun is approximately 4.6 billion years old. This age is estimated based on the study of solar system formation and the ages of the oldest meteorites.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping a game\n",
    "There are two important elements that need to be considered:\n",
    "##### MOVE_RULE\n",
    "this somewhat defines, what a valid move (what form it takes) is\n",
    "\n",
    "##### GAME_RULE\n",
    "This contains rules that determine what effect a valid move from above should have on the game state, whether a move is legal, whether it updates the game state to `finished`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Game\n",
    "Player A and B take turns, while they have a conversation. Every first word has to start with the next letter of the alphabet, based on the first word of the previous turn and so on, while sticking to a specific topic.\n",
    "\n",
    "\n",
    "### Player A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_A = Template('''Let us play a game. I will tell you a topic and you will give me a short sentence that fits to this topic.\n",
    "But I will also tell you a letter. The sentence that you give me has to start with this letter.\n",
    "After you have answered, I will give you my reply, which will start with the letter following your letter in the alphabet.\n",
    "Then it is your turn again, to produce a reply starting with the letter following that one. And so on. Let's go.\n",
    "Start your utterance with I SAY: and do no produce any other text.\n",
    "The topic is: $topic\n",
    "The letter is: $letter''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let us play a game. I will tell you a topic and you will give me a short sentence that fits to this topic.\\nBut I will also tell you a letter. The sentence that you give me has to start with this letter.\\nAfter you have answered, I will give you my reply, which will start with the letter following your letter in the alphabet.\\nThen it is your turn again, to produce a reply starting with the letter following that one. And so on. Let's go.\\nStart your utterance with I SAY: and do no produce any other text.\\nThe topic is: clothes\\nThe letter is: b\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic = 'clothes'\n",
    "letter = 'b'\n",
    "# Substitute here goes into the init_prompt_A template and replaces $topic and $letter with the variables defined above\n",
    "# This will be logic every single time we call this below\n",
    "init_prompt_A.substitute(topic=topic, letter=letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happens here?\n",
    "So now the initial prompt is defined, along with the topic and the starting letter. <br> Understanding what is happening here is easier, if you think about this part as the start of a conversation with a language model like **ChatGPT**. <br> `init_prompt_A` would basically be your first message to ChatGPT. See: <br> <br> \n",
    "\n",
    "<img src=\"../../utils/gpt_first_prompt.png\" width=\"1200\" style=\"border: 2px solid black;\" />\n",
    "\n",
    "<br clear=\"both\"/>\n",
    "\n",
    "Our goal with the game is that we implement a GameMaster that includes `GAME_RULE` and `MOVE_RULE`, which allow us to actually control the flow of the game <br>\n",
    "We could not do that, if we were to just continue the game with ChatGPT in the picture above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I SAY: Blue jeans are a timeless wardrobe staple.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_A, resp, resp_text = llm.generate_response(add_message([], init_prompt_A.substitute(topic='clothes', letter='b')))\n",
    "resp_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding MOVE_RULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we defined in the initial prompt, what our LM returns, has to start with the prefix 'I SAY'\n",
    "# If it is not the case, the function returns False = Wrong guess (?)\n",
    "prefix = 'I SAY: '\n",
    "def parse_reply(text, prefix=prefix):\n",
    "    if not text.startswith(prefix):\n",
    "        return False\n",
    "    return text[len(prefix):]\n",
    "    # The text is returned, but without the prefix (the length of the prefix is 'cut' from the string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what `parse_reply` returns. <br>\n",
    "If we have a sentence without `'I SAY'`, that means, the prompt worked and the LM produced a valid guess.\n",
    "<br> If we see **False**, then the guess is incorrect, or the LM did not understand the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Blue jeans are a timeless wardrobe staple.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_reply(resp_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding GAME_RULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function checks, whether the first letter of the first word in the tokenized text is the same as the 'letter' argument\n",
    "def check_move(text, letter):\n",
    "    token = text.split()\n",
    "    if token[0][0].lower() == letter: # and token[-1][0].lower() == letter:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_move(parse_reply(resp_text), letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player B:\n",
    "This part is pretty self-explanatory, the prompt is a bit different, than what is given to Player A, as there was an initial turn already. <br> \n",
    "That means we already have a sentence that has to be passed on to player B as part of this prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_B = Template('''Let us play a game. I will give you a sentence.\n",
    "The first word in my sentence starts with a certain letter.\n",
    "I want you to give me a sentence as a reply, with the same topic as my sentence, but different from my sentence.\n",
    "The first word of your sentence should start with the next letter in the alphabet from the letter my sentence started with.\n",
    "Let us try to have a whole conversation like that.\n",
    "Please start your reply with 'I SAY:' and do not produce any other text.\n",
    "Let's go.\n",
    "My sentence is: $sentence\n",
    "What do you say?''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let us play a game. I will give you a sentence.\\nThe first word in my sentence starts with a certain letter.\\nI want you to give me a sentence as a reply, with the same topic as my sentence, but different from my sentence.\\nThe first word of your sentence should start with the next letter in the alphabet from the letter my sentence started with.\\nLet us try to have a whole conversation like that.\\nPlease start your reply with 'I SAY:' and do not produce any other text.\\nLet's go.\\nMy sentence is: Blue jeans are a timeless wardrobe staple.\\nWhat do you say?\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = parse_reply(resp_text) # once again, the first response is validated and passed on as the argument 'sentence'\n",
    "prompt_B = init_prompt_B.substitute(sentence=sentence)\n",
    "prompt_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I SAY: Classic trousers can elevate any outfit effortlessly.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now an actual response is generated. The functionality here is the same as for Player A\n",
    "prompt_B, resp_B, resp_B_text = llm.generate_response(add_message([], prompt_B))\n",
    "resp_B_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Classic trousers can elevate any outfit effortlessly.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Response is validated and the prefix 'I SAY' gets ommited.\n",
    "parse_reply(resp_B_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Game States\n",
    "Now that the initial prompts have been defined, the state of the game has to be handled. We need to validate the Move. If it is correct, the game state is updated, until a finishing level is met. If the move does not valide, the game is aborted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialsGameState():\n",
    "    def __init__(self, letter):\n",
    "        self.letter = letter\n",
    "        self.n_moves = 0\n",
    "        self.success = False\n",
    "        self.aborted = False\n",
    "    \n",
    "    # Update the letter after a successful turn\n",
    "    def increment_state(self):\n",
    "        self.letter = chr(ord(self.letter) + 1 )\n",
    "        self.n_moves += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('b', 0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_game = InitialsGameState('b') # Create an Instance of the GameState Class\n",
    "# this_game.increment_state() # Updating to next letter in the alphabet\n",
    "this_game.letter, this_game.n_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the first letter of the parsed response matches the class attribute letter\n",
    "check_move(parse_reply(resp_B_text), this_game.letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Game Loop\n",
    "From here on, the game consists of giving A the turn, parsing and validating the response, then the same with B. The game has to be stopped, if: <br>\n",
    "- Unparseable move detected (not understandable response) <br>\n",
    "- A losing move was made (wrong initial) <br>\n",
    "- Maximum number of turns reached\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': [{'type': 'text', 'text': 'You are a player in a game'}]},\n",
       " {'role': 'user',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': \"Let us play a game. I will tell you a topic and you will give me a short sentence that fits to this topic.\\nBut I will also tell you a letter. The sentence that you give me has to start with this letter.\\nAfter you have answered, I will give you my reply, which will start with the letter following your letter in the alphabet.\\nThen it is your turn again, to produce a reply starting with the letter following that one. And so on. Let's go.\\nStart your utterance with I SAY: and do no produce any other text.\\nThe topic is: clothes\\nThe letter is: b\"}]},\n",
       " {'role': 'assistant',\n",
       "  'content': 'I SAY: Blue jeans are a timeless wardrobe staple.'},\n",
       " {'role': 'user',\n",
       "  'content': 'I SAY: Classic trousers can elevate any outfit effortlessly.'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_prompt_A = add_message(prompt_A, resp_text, role='assistant')\n",
    "next_prompt_A = add_message(next_prompt_A, resp_B_text, role='user')\n",
    "next_prompt_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I SAY: Blue jeans are a timeless wardrobe staple.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAY\n"
     ]
    }
   ],
   "source": [
    "psd_reply = parse_reply(resp_text)\n",
    "if psd_reply:\n",
    "    if check_move(psd_reply, letter=this_game.letter):\n",
    "        print('YAY')\n",
    "    else:\n",
    "        print('LOST')\n",
    "else:\n",
    "    print('NOT WELL-FORMED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': [{'type': 'text', 'text': 'You are a player in a game'}]},\n",
       " {'role': 'user',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': \"Let us play a game. I will give you a sentence.\\nThe first word in my sentence starts with a certain letter.\\nI want you to give me a sentence as a reply, with the same topic as my sentence, but different from my sentence.\\nThe first word of your sentence should start with the next letter in the alphabet from the letter my sentence started with.\\nLet us try to have a whole conversation like that.\\nPlease start your reply with 'I SAY:' and do not produce any other text.\\nLet's go.\\nMy sentence is: Blue jeans are a timeless wardrobe staple.\\nWhat do you say?\"}]},\n",
       " {'role': 'assistant',\n",
       "  'content': 'I SAY: Classic trousers can elevate any outfit effortlessly.'},\n",
       " {'role': 'user',\n",
       "  'content': 'I SAY: Blue jeans are a timeless wardrobe staple.'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'I SAY: Classic trousers can elevate any outfit effortlessly.'},\n",
       " {'role': 'user',\n",
       "  'content': 'I SAY: Blue jeans are a timeless wardrobe staple.'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_prompt_B = add_message(prompt_B, resp_B_text, role='assistant')\n",
    "next_prompt_B = add_message(next_prompt_B, resp_text, role='user')\n",
    "next_prompt_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-28 17:31:42,858 - backends.openai_api - WARNING - 'list' object has no attribute 'replace', retrying in 0 seconds...\n",
      "2024-08-28 17:31:42,864 - backends.openai_api - WARNING - 'list' object has no attribute 'replace', retrying in 0 seconds...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prompt_B, resp_B, resp_B_text \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_prompt_B\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m resp_B_text\n",
      "File \u001b[0;32m~/Desktop/Gruppe-C-PRO2-Projekt/venv/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Gruppe-C-PRO2-Projekt/venv/lib/python3.10/site-packages/retry/api.py:73\u001b[0m, in \u001b[0;36mretry.<locals>.retry_decorator\u001b[0;34m(f, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m args \u001b[38;5;241m=\u001b[39m fargs \u001b[38;5;28;01mif\u001b[39;00m fargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m     72\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m fkwargs \u001b[38;5;28;01mif\u001b[39;00m fkwargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__retry_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexceptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_delay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackoff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjitter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Gruppe-C-PRO2-Projekt/venv/lib/python3.10/site-packages/retry/api.py:33\u001b[0m, in \u001b[0;36m__retry_internal\u001b[0;34m(f, exceptions, tries, delay, max_delay, backoff, jitter, logger)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m _tries:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     35\u001b[0m         _tries \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Gruppe-C-PRO2-Projekt/clembench-main/backends/utils.py:60\u001b[0m, in \u001b[0;36mensure_messages_format.<locals>.wrapped_fn\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(generate_response_fn)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages):\n\u001b[1;32m     59\u001b[0m     _messages \u001b[38;5;241m=\u001b[39m ensure_alternating_roles(messages)\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerate_response_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_messages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Gruppe-C-PRO2-Projekt/clembench-main/backends/openai_api.py:97\u001b[0m, in \u001b[0;36mOpenAIModel.generate_response\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;129m@retry\u001b[39m(tries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, delay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;129m@ensure_messages_format\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages: List[Dict]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, Any, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    :param messages: for example\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m            [\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    :return: the continuation\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     api_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_spec\u001b[38;5;241m.\u001b[39mmodel_id,\n\u001b[1;32m    100\u001b[0m                                                        messages\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m    101\u001b[0m                                                        temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_temperature(),\n\u001b[1;32m    102\u001b[0m                                                        max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_max_tokens())\n\u001b[1;32m    103\u001b[0m     message \u001b[38;5;241m=\u001b[39m api_response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Desktop/Gruppe-C-PRO2-Projekt/clembench-main/backends/openai_api.py:59\u001b[0m, in \u001b[0;36mOpenAIModel.encode_messages\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m     52\u001b[0m encoded_messages \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages:\n\u001b[1;32m     55\u001b[0m     this \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: message[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     56\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     57\u001b[0m                 {\n\u001b[1;32m     58\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m---> 59\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mmessage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m <image> \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m                 }\n\u001b[1;32m     61\u001b[0m             ]}\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_spec\u001b[38;5;241m.\u001b[39mhas_attr(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_images\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "prompt_B, resp_B, resp_B_text = llm.generate_response(next_prompt_B)\n",
    "resp_B_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_game.increment_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classic trousers can elevate any outfit effortlessly.\n",
      "b\n",
      "LOST\n"
     ]
    }
   ],
   "source": [
    "psd_reply = parse_reply(resp_B_text)\n",
    "if psd_reply:\n",
    "    if check_move(psd_reply, letter=this_game.letter):\n",
    "        print('YAY')\n",
    "    else:\n",
    "        print('LOST')\n",
    "else:\n",
    "    print('NOT WELL-FORMED')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
