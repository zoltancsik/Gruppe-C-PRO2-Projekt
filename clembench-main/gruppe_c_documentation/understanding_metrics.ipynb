{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Metrics\n",
    "### Disclaimer\n",
    "This notebook closely follows the content from **clemgame/metrics.py** in the [**clp-research/clembench**](https://github.com/clp-research/clembench) repository. <br> \n",
    "While the core content is derived from the original file, some additional detailed descriptions and explanations have been added to serve as a source of knowledge during the development of our project.\n",
    "___\n",
    "During the implementation of the game, the exact same names should be used to ensure functionality <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`METRIC_ABORTED = \"Aborted\"`** \n",
    "- Record Level: Episode\n",
    "- 1 or 0, depending on whether the game was aborted\n",
    "- It does not include games lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "if aborted:\n",
    "    self.log_episode_score(ms.METRIC_ABORTED, 1)\n",
    "    # Where ms refers to backends/metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`METRIC_LOSE = \"Lose\"`**\n",
    "- Record Level: Episode\n",
    "- **0** or **1**, depending on whether the game is lost.\n",
    "- Does not include Abort\n",
    "- Opposite of success\n",
    "- This is always **0** if the game was aborted (!=lost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example where game is aborted (!= lost)\n",
    "if aborted:\n",
    "    self.log_episode_score(ms.METRIC_ABORTED, 1)\n",
    "    self.log_episode_score(ms.METRIC_SUCCESS, 0)\n",
    "    self.log_episode_score(ms.METRIC_LOSE, 0)\n",
    "\n",
    "# Example where game is lost, (!=aborted|!=success)\n",
    "if success_a != success_b:\n",
    "                self.log_episode_score(ms.METRIC_ABORTED, 0)\n",
    "                self.log_episode_score(ms.METRIC_SUCCESS, 0)\n",
    "                self.log_episode_score(ms.METRIC_LOSE, 1)\n",
    "                # Game-specific metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`METRIC_SUCCESS = \"Success\"`**\n",
    "- Episode or Turn level\n",
    "- 0 or 1, depending on whether the gameplay was successful (goal reached in x amount of turns)\n",
    "- Opposite of Lost \n",
    "- Always 0 if the game was aborted (!=lost|!=success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "if success_a and success_b:   \n",
    "    self.log_episode_score(ms.METRIC_ABORTED, 0)\n",
    "    self.log_episode_score(ms.METRIC_SUCCESS, 1)\n",
    "    self.log_episode_score(ms.METRIC_LOSE, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`BENCH_SCORE = \"Main Score\"`**\n",
    "- The main score of the game. \n",
    "- It is a value between **0** and **100** that summarizes overall performance\n",
    "- Episode level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "if success_a and success_b:   \n",
    "    self.log_episode_score(ms.METRIC_ABORTED, 0)\n",
    "    self.log_episode_score(ms.METRIC_SUCCESS, 1)\n",
    "    self.log_episode_score(ms.METRIC_LOSE, 0)\n",
    "    # Game-specific metrics\n",
    "    self.log_episode_score(ms.BENCH_SCORE, 100)\n",
    "    self.log_episode_score(\"Player Score\", 100)\n",
    "else:\n",
    "    self.log_episode_score(ms.BENCH_SCORE, 0)\n",
    "    self.log_episode_score(\"Player Score\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### API Requests\n",
    "**`METRIC_REQUEST_COUNT = \"Request Count\"`**\n",
    "- How many requests were made to the API\n",
    "- Can be Episode or Turn Level\n",
    "\n",
    "**`METRIC_REQUEST_COUNT_PARSED = \"Parsed Request Count\"`**\n",
    "- How many calls were made during the whole game play that were actually successfully parsed\n",
    "- Episode or optionally turn level\n",
    "\n",
    "**`METRIC_REQUEST_COUNT_VIOLATED = \"Violated Request Count\"`**\n",
    "- How many requests were made during the whole gameplay that could not be parsed.\n",
    "- Episode or optionally turn level\n",
    "\n",
    "**`METRIC_REQUEST_SUCCESS = \"Request Success Ratio\"`**\n",
    "- **METRIC_REQUEST_COUNT_PARSED/METRIC_REQUEST_COUNT**\n",
    "- Episode or optionally turn level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "def score_requests(self, episode_interactions: Dict):\n",
    "    # logging total request count, parsed, violated, and success ratio of parsed requests over all requests\n",
    "    request_count = episode_interactions[\n",
    "        ms.METRIC_REQUEST_COUNT]  # could also be calculated by adding parsed and violated requests\n",
    "    parsed_requests = episode_interactions[ms.METRIC_REQUEST_COUNT_PARSED]\n",
    "    violated_requests = episode_interactions[ms.METRIC_REQUEST_COUNT_VIOLATED]\n",
    "\n",
    "    self.log_episode_score(ms.METRIC_REQUEST_COUNT, request_count)\n",
    "    self.log_episode_score(ms.METRIC_REQUEST_COUNT_PARSED, parsed_requests)\n",
    "    self.log_episode_score(ms.METRIC_REQUEST_COUNT_VIOLATED, violated_requests)\n",
    "    self.log_episode_score(ms.METRIC_REQUEST_SUCCESS, parsed_requests / request_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Try to use all of these Metrics when Implementing our game\n",
    "- Some of these will have to be fine-tuned, changed in our version\n",
    "- Potentially necessary: adding new metrics."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
