{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Add a New Game\n",
    "\n",
    "### Disclaimer\n",
    "This notebook closely follows the tutorial steps from [docs/howto_add_games.md](https://github.com/clp-research/clembench/blob/main/docs/howto_add_games.md)\n",
    " in the [**clp-research/clembench**](https://github.com/clp-research/clembench) repository. <br> \n",
    "While the core content is derived from the original tutorial, some additional detailed descriptions and explanations have been added to serve as a source of knowledge during the development of our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The GameBenchMark Class\n",
    "The Benchmark is run for a particular game with **python scripts/cli.py run -g game -m model** <br>\n",
    "- If only one model is defined for a two player game, it will represent both players.\n",
    "\n",
    "## Execution\n",
    "- When the run command is executed, the run routine in `benchmark.py` will determine the game code that needs to be invoked.\n",
    "- The benchmark code loads all subclasses that inherit from GameBenchMark and calls `setup` on them.\n",
    "- The setup method already loads the game instances **-->** every subclass is asked if it applies to the given name.\n",
    "- That means, there has to be a subclass like the following for each game\n",
    "- The subclass provides `GAME_NAME=taboo` and the rest is taken care of by the SuperClass\n",
    "- Then the benchmark code checks if your game is single or multiplayer game (the default is multi-player).\n",
    "- Then the run(dialog_pair,temperature) method is called by GameBenchmark.\n",
    "- This is when the GameMaster becomes relevant (which is returned by thr `create_game_master()` factory method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabooGameBenchmark(GameBenchmark):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(GAME_NAME)\n",
    "\n",
    "    def get_description(self):\n",
    "        return \"Taboo game between two agents where one has to describe a word for the other to guess.\"\n",
    "\n",
    "    def create_game_master(self, experiment: Dict, player_backends: List[str]) -> GameMaster:\n",
    "        return Taboo(experiment, player_backends)\n",
    "        \n",
    "    def is_single_player(self) -> bool:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `get_description method`\n",
    "This is returned when `python3 scripts/cli.py ls` is executed, see: <br>\n",
    "```\n",
    "2024-08-31 12:55:58,735 - benchmark.run - INFO - Listing benchmark games:\n",
    "2024-08-31 12:55:58,736 - benchmark.run - INFO -  Game: cloudgame -> A simple game in which a player has to decide whether they see clouds or not.\n",
    "2024-08-31 12:55:58,736 - benchmark.run - INFO -  Game: imagegame -> Image Game simulation to generate referring expressions and fill a grid accordingly\n",
    "2024-08-31 12:55:58,736 - benchmark.run - INFO -  Game: matchit -> A simple game in which two players have to decide whether they see the same image or not.\n",
    "2024-08-31 12:55:58,736 - benchmark.run - INFO -  Game: matchit -> A simple game in which two players have to decide whether they see the same image or not.\n",
    "2024-08-31 12:55:58,736 - benchmark.run - INFO -  Game: matchit -> A simple game in which two players have to decide whether they see the same image or not.\n",
    "2024-08-31 12:55:58,736 - benchmark.run - INFO -  Game: matchit -> A simple game in which two players have to decide whether they see the same image or not.\n",
    "2024-08-31 12:55:58,737 - benchmark.run - INFO -  Game: privateshared -> Questioner and answerer in scorekeeping game.\n",
    "2024-08-31 12:55:58,737 - benchmark.run - INFO -  Game: taboo -> Taboo game between two agents where one has to describe a word for the other to guess.\n",
    "2024-08-31 12:55:58,737 - benchmark.run - INFO -  Game: textmapworld -> Graph Game.\n",
    "2024-08-31 12:55:58,737 - benchmark.run - INFO -  Game: textmapworld_description -> Graph Game.\n",
    "2024-08-31 12:55:58,737 - benchmark.run - INFO -  Game: textmapworld_graphreasoning -> Graph Game.\n",
    "2024-08-31 12:55:58,737 - benchmark.run - INFO -  Game: textmapworld_questions -> Graph Game.\n",
    "2024-08-31 12:55:58,738 - benchmark.run - INFO -  Game: textmapworld_specificroom -> Graph Game.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GameMaster Class\n",
    "- For each `experiment`in instances.json, that has been loaded with `on_setup()`, the game benchmark code applies the given dialog pair (or if not provided, tries to determine it based on instance information) <br>\n",
    "- Each experiment represents a specific condition for the game, like difficulity and holds the actual game instance. <br>\n",
    "- A `GameMaster` is created for each instance, by `self.create_gamemaster` method of the BenchMark. <br>\n",
    "- The `GameMaster`is in charge of playing a single isntance of the game <br>\n",
    "- For Taboo: Target word to be guessed, words that are not allowed to be said."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "   game_master = self.create_game_master(experiment_config, dialogue_pair)\n",
    "   # Recieve Game Information\n",
    "   game_master.setup(**game_instance)\n",
    "   # Coordinate actual Play\n",
    "   game_master.play()\n",
    "   # Stores the interactions between the players and GM in game_record_dir\n",
    "   game_master.store_records(game_id, game_record_dir)\n",
    "except Exception:  # continue with other episodes if something goes wrong\n",
    "   self.logger.exception(f\"{self.name}: Exception for episode {game_id} (but continue)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Classes\n",
    "\n",
    "#### MyBenchMark - extending GameBenchMark\n",
    "Following Methods are necessary to implement: <br>\n",
    "- `__init__(self)`, that calls on `super().__init__(GAME_NAME)`\n",
    "- `get_description`- explained above\n",
    "- `is_single_player(self) -> bool` - determines if one player is sufficient\n",
    "- `create_game_master(self, experiment: Dict, player_backends: List[str]) -> MyGameMaster` that returns **MyGameMaster**\n",
    "<br>\n",
    "\n",
    "### MyGameMaster that extends GameMaster\n",
    "- `__init__(self, name: str, experiment: Dict, player_backends: List[str] = None):` \n",
    "    - Receives the experiment information and the players that play the game.\n",
    "    - These can be simply delegated to `super()`.\n",
    "- `setup(self, **game_instance)` which sets the information specified in `instances.json`\n",
    "- `play(self)`which executes the game logic and performs turns in the game\n",
    "- `compute_scores` which is called when the user executes `python3 scripts/cli.py score taboo`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DialogueGameMaster\n",
    "`MyGameMaster` can implement play(), but in some cases we can extend from `DialogueGameMaster`, which is a more concrete subclass of `GameMaster`. <br>\n",
    "It defines the play routine as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(self) -> None:\n",
    "     self._on_before_game()\n",
    "     while self._does_game_proceed():\n",
    "         self.log_next_turn()  # not sure if we want to do this always here (or add to _on_before_turn)\n",
    "         self._on_before_turn(self.current_turn)\n",
    "         self.logger.info(f\"{self.name}: %s turn: %d\", self.name, self.current_turn)\n",
    "         for player in self.__player_sequence():\n",
    "             if not self._does_game_proceed():\n",
    "                 break  # potentially stop in between player turns\n",
    "             # GM -> Player\n",
    "             history = self.messages_by_names[player.descriptor]\n",
    "             assert history, f\"messages history must not be empty for {player.descriptor}\"\n",
    "\n",
    "             last_entry = history[-1]\n",
    "             assert last_entry[\"role\"] != \"assistant\", \"Last entry should not be assistant \" \\\n",
    "                                                       \"b.c. this would be the role of the current player\"\n",
    "             message = last_entry[\"content\"]\n",
    "\n",
    "             action = {'type': 'send message', 'content': message}\n",
    "             self.log_event(from_='GM', to=player.descriptor, action=action)\n",
    "\n",
    "             _prompt, _response, response_message = player(history, self.current_turn)\n",
    "\n",
    "             # Player -> GM\n",
    "             action = {'type': 'get message', 'content': response_message}\n",
    "             self.log_event(from_=player.descriptor, to=\"GM\", action=action, call=(_prompt, _response))\n",
    "\n",
    "             # GM -> GM\n",
    "             self.__validate_parse_and_add_player_response(player, response_message)\n",
    "         self._on_after_turn(self.current_turn)\n",
    "         self.current_turn += 1\n",
    "     self._on_after_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as `_does_game_proceed():`\n",
    "- **GM --> Player:**\n",
    "    1. At a player's turn, the player recieves its view on the history of messages (`messages_by_names`)\n",
    "    2. The last message is logged (`log_event`) as a GM->Player event in the interactions log. \n",
    "    3. Then player is asked to create a response based on the history and current turn index.\n",
    "- **Player --> Game:**\n",
    "    1. The response is recieved and logged as Player --> GM in the event log\n",
    "- **GM --> GM**\n",
    "    1. Validates and stores response if valid, then goes to next turn\n",
    "\n",
    "This shows that the logging is systematically done when using DialogueGameMaster. <br>\n",
    "There are however several ways to customize the gameplay:\n",
    "\n",
    "- `def _on_setup(self, **kwargs)` which must be implemented. Use add_player() here to add the players.\n",
    "- `def _does_game_proceed(self) -> bool` which must be implemented. Decides if the game can continue.\n",
    "- `def _validate_player_response(self, player: Player, utterance: str) -> bool` to decide if an utterance should be added. This is also the place to check for game end conditions.\n",
    "- `def _on_parse_response(self, player: Player, utterance: str) -> Tuple[str, bool]` to decide if a response utterance should be modified. If not simply return the utterance.\n",
    "- `def _after_add_player_response(self, player: Player, utterance: str)` to add the utterance to other player's history, if necessary. To do this use the method add_user_message(other_player,utterance).\n",
    "- the general game hooks `_on_before_game()` and `_on_after_game()`\n",
    "- the general turn hooks `_on_before_turn(turn_idx)` and `_on_after_turn(turn_idx)`\n",
    "\n",
    "### Setup Taboo Game\n",
    "The setup hook is used to set instance speciifc values and to setup the **WordDescriber** and **WordGuesser** which are the Player for the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _on_setup(self, **game_instance):\n",
    "    logger.info(\"_on_setup\")\n",
    "    self.game_instance = game_instance\n",
    "\n",
    "    self.describer = WordDescriber(self.player_models[0], self.max_turns)\n",
    "    self.guesser = WordGuesser(self.player_models[1])\n",
    "\n",
    "    self.add_player(self.describer)\n",
    "    self.add_player(self.guesser)\n",
    "\n",
    "# General game hook is used to set the initial prompts for both players\n",
    "def _on_before_game(self):\n",
    "  self.add_user_message(self.describer, self.describer_initial_prompt)\n",
    "  self.add_user_message(self.guesser, self.guesser_initial_prompt)\n",
    "\n",
    "# Then it has to be decided if the guessing should continue\n",
    "def _does_game_proceed(self):\n",
    "    if self.invalid_response:\n",
    "        self.log_to_self(\"invalid format\", \"abort game\")\n",
    "        return False\n",
    "    if self.clue_error is not None:\n",
    "        return False \n",
    "    if self.current_turn >= self.max_turns:\n",
    "        self.log_to_self(\"max turns reached\", str(self.max_turns))\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Then check if the player response is in valid format. (MOVE_RULE)\n",
    "def _validate_player_response(self, player, utterance: str) -> bool:\n",
    "  if player == self.guesser:\n",
    "      if not utterance.startswith(\"GUESS:\"):\n",
    "          self.invalid_response = True\n",
    "          return False\n",
    "  if player == self.describer:\n",
    "      if not utterance.startswith(\"CLUE:\"):\n",
    "          self.invalid_response = True\n",
    "          return False\n",
    "      errors = check_clue(utterance, self.target_word, self.related_words)\n",
    "      if errors:\n",
    "          error = errors[0]\n",
    "          self.clue_error = error\n",
    "          return False\n",
    "  self.log_to_self(\"valid format\", \"continue\")\n",
    "  return True\n",
    "\n",
    "# This is where we can detect invalid MOVEs or log reponses without prefixes:\n",
    "def _on_parse_response(self, player, utterance: str) -> Tuple[str, bool]:\n",
    "  if player == self.guesser:\n",
    "      utterance = utterance.replace(\"GUESS:\", \"\")\n",
    "      self.guess_word = utterance.lower()\n",
    "      self.log_to_self(\"guess\", self.guess_word)\n",
    "  if player == self.describer:\n",
    "      utterance = utterance.replace(\"CLUE:\", \"\")\n",
    "      self.log_to_self(\"clue\", utterance)\n",
    "  return utterance, False\n",
    "\n",
    "# The (modified) response is then added to the player's history:\n",
    "def _after_add_player_response(self, player, utterance: str):\n",
    "    if player == self.describer:\n",
    "        utterance = f\"CLUE: {utterance}.\"\n",
    "        self.add_user_message(self.guesser, utterance)\n",
    "    if player == self.guesser:\n",
    "        if self.guess_word != self.target_word:\n",
    "            utterance = f\"GUESS: {self.guess_word}.\"\n",
    "            self.add_user_message(self.describer, utterance)\n",
    "\n",
    "# Finally, use general turn method to additionally log the initial prompt for the second player \n",
    "# and not only the most recent one (as automatically done by GameMaster)\n",
    "def _on_before_turn(self, turn_idx: int):\n",
    "    if turn_idx == 0:\n",
    "        self.log_message_to(self.guesser, self.guesser_initial_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GameResourceLocator\n",
    "Provides methods to access, load and store files from within the game directory. <br>\n",
    "When implementing the game, always use the methods of this class to handle files.\n",
    "- **Usage:** \n",
    "    - `gm.load_json(\"my_file\")` located directly in the game's directory\n",
    "    - `gm.load_json(\"sub/my_file\")` in `game/sub/my_file.json` to acces subdirectories.\n",
    "\n",
    "**Expected Game Structure:**\n",
    "~~~\n",
    "games\n",
    "├──mygame\n",
    "│     ├── in\n",
    "│     │   └── instances.json\n",
    "│     ├── resources\n",
    "│     │   └── initial_prompt.template\n",
    "│     ├── instancegenerator.py\n",
    "│     └── master.py\n",
    "...\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player Class\n",
    "A player object recieves messages and returns a textual response. <br>\n",
    "- The response can be either from an api to a cLLM with the `__call__`method, or a predefined `_custom_response`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clemgame.clemgame import Player\n",
    "\n",
    "class WordGuesser(Player):\n",
    "\n",
    "   def __init__(self, model_name):\n",
    "      super().__init__(model_name)\n",
    "\n",
    "   def _custom_response(self, messages, turn_idx):\n",
    "      # mock response\n",
    "      return f'Pear'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GameInstanceGenerator\n",
    "- To let agents play a game, we need to have a description that instantiates single episodes\n",
    "- **Taboo:** Each episode is played with a specific target word that also comes with a list of other, related and forbidden words.\n",
    "- The Class generates full instances that include initial propmts for the models and other metha information for running experiments.\n",
    "- **Example for Taboo:**\n",
    "    - Use word list of 3 frequencies (low/medium/high)\n",
    "    - Test 3 LLMs (Taboo is played by 2 LLMs)\n",
    "    - Fix the number of maximum turns\n",
    "    - Generate fix number of instances\n",
    "- Instances are generated as a JSON in `games/game_name/in/instances.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clemgame.clemgame import GameInstanceGenerator\n",
    "\n",
    "N_INSTANCES = 20  # how many different target words; zero means \"all\"\n",
    "N_GUESSES = 3  # how many tries the guesser will have\n",
    "N_REATED_WORDS = 3\n",
    "LANGUAGE = \"en\"\n",
    "\n",
    "class TabooGameInstanceGenerator(GameInstanceGenerator):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"taboo\")\n",
    "\n",
    "    def on_generate(self):\n",
    "        player_assignments = list(itertools.permutations([OpenAI.MODEL_GPT_35, Anthropic.MODEL_CLAUDE_13]))\n",
    "        for difficulty in [\"low\", \"medium\", \"high\"]:\n",
    "\n",
    "            # first choose target words based on the difficultly\n",
    "            fp = f\"resources/target_words/{LANGUAGE}/{difficulty}_freq_100\"\n",
    "            target_words = self.load_file(file_name=fp, file_ending=\".txt\").split('\\n')\n",
    "            if N_INSTANCES > 0:\n",
    "                assert len(target_words) >= N_INSTANCES, \\\n",
    "                    f'Fewer words available ({len(target_words)}) than requested ({N_INSTANCES}).'\n",
    "                target_words = random.sample(target_words, k=N_INSTANCES)\n",
    "\n",
    "            # use the same target_words for the different player assignments\n",
    "            experiment = self.add_experiment(f\"{difficulty}_{LANGUAGE}\", dialogue_partners=player_assignments)\n",
    "            experiment[\"max_turns\"] = N_GUESSES\n",
    "\n",
    "            describer_prompt = self.load_template(\"resources/initial_prompts/initial_describer\")\n",
    "            guesser_prompt = self.load_template(\"resources/initial_prompts/initial_guesser\")\n",
    "            experiment[\"describer_initial_prompt\"] = describer_prompt\n",
    "            experiment[\"guesser_initial_prompt\"] = guesser_prompt\n",
    "\n",
    "            for game_id in tqdm(range(len(target_words))):\n",
    "                target = target_words[game_id]\n",
    "\n",
    "                game_instance = self.add_game_instance(experiment, game_id)\n",
    "                game_instance[\"target_word\"] = target\n",
    "                game_instance[\"related_word\"] = []\n",
    "\n",
    "                if len(game_instance[\"related_word\"]) < N_REATED_WORDS:\n",
    "                    print(f\"Found less than {N_REATED_WORDS} related words for: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Experiments with games\n",
    "- Adding the `-e` argument to the execution of `cli.py` declares that an experiment is to be done.\n",
    "- This creates a **results** folder with the following structure: <br>\n",
    "    - Directories that mention the involved models\n",
    "        - i.e.: gpt-3.5-turbo-1106-t0.0--gpt-3.5-turbo-1106-t0.0\n",
    "    - Directory structure for each episode (based on instances.json):\n",
    "        - **instance.json**\n",
    "        - **interaction.json**\n",
    "        - **transcript.html**\n",
    "    - experiment_name.json, that contains the run parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface Prototyping Check Methods\n",
    "The huggingface-local backend offers two functions to check messages lists that clemgames might pass to the backend without the need to load the full model weights.  <br>\n",
    "Allows to prototype clemgames locally with minimal hardware demand.\n",
    "\n",
    "### Messages Checking\n",
    "The `check_messages` function in `backends/huggingface_local_api.py` takes a messages list and a ModelSpec as arguments. <br>\n",
    "- Prints all anticipated issues with the passed messages list to console if they occur. <br> \n",
    "- Applies the given model's chat template to the messages as a direct check. <br>\n",
    "- Returns **False** if the chat template does not accept the messages and prints the outcome to console.\n",
    "\n",
    "### Context Limit Checking\n",
    "The `check_context_limit` takes a messages list and a ModelSpec as required arguments.\n",
    "- Further arguments:\n",
    "    - number of tokens to generate max_new_tokens: int (default: 100), \n",
    "    - clean_messages: bool (default: False) to apply message cleaning as the generation method will,\n",
    "    - verbose: bool (default: True) for console printing of the values.\n",
    "- Prints:\n",
    "    - The token count for the passed messages after chat template application,\n",
    "    - The remaining number of tokens (negative if context limit is exceeded)\n",
    "    - Maximum number of tokens the model allows as generation input.\n",
    "- Returns a tuple with four elements:\n",
    "    - **bool:** True if context limit was not exceeded, False if it was.\n",
    "    - **int:** number of tokens for the passed messages.\n",
    "    - **int:** number of tokens left in context limit.\n",
    "    - **int:** context token limit."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
